{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "south-valve",
   "metadata": {},
   "source": [
    "https://github.com/Beckschen/TransUNet/blob/main/datasets/dataset_synapse.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-sessions",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-mambo",
   "metadata": {},
   "source": [
    "## Auxiliary functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rot_flip(image, label):\n",
    "    k = np.random.randint(0, 4)\n",
    "    image = np.rot90(image, k)\n",
    "    label = np.rot90(label, k)\n",
    "    axis = np.random.randint(0, 2)\n",
    "    image = np.flip(image, axis=axis).copy()\n",
    "    label = np.flip(label, axis=axis).copy()\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def random_rotate(image, label):\n",
    "    angle = np.random.randint(-20, 20)\n",
    "    image = ndimage.rotate(image, angle, order=0, reshape=False)\n",
    "    label = ndimage.rotate(label, angle, order=0, reshape=False)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "class RandomGenerator(object):\n",
    "    def __init__(self, output_size):\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        if random.random() > 0.5:\n",
    "            image, label = random_rot_flip(image, label)\n",
    "        elif random.random() > 0.5:\n",
    "            image, label = random_rotate(image, label)\n",
    "        x, y = image.shape\n",
    "        if x != self.output_size[0] or y != self.output_size[1]:\n",
    "            image = zoom(image, (self.output_size[0] / x, self.output_size[1] / y), order=3)  # why not 3?\n",
    "            label = zoom(label, (self.output_size[0] / x, self.output_size[1] / y), order=0)\n",
    "        image = torch.from_numpy(image.astype(np.float32)).unsqueeze(0)\n",
    "        label = torch.from_numpy(label.astype(np.float32))\n",
    "        sample = {'image': image, 'label': label.long()}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-boards",
   "metadata": {},
   "source": [
    "---\n",
    "## temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "seasonal-breath",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Nimgs = 20\n",
    "channels = 3\n",
    "height = 584\n",
    "width = 565\n",
    "save_data_dir = \"./DRIVE_datasets_training_testing/\"\n",
    "\n",
    "#------------Path of the images -----------------------------\n",
    "self.original_base_dir = '/home/staff/azad/deeplearning/datasets/DRIVE'\n",
    "#train\n",
    "self.original_imgs_train_dir = f\"{self.original_base_dir}/training/images/\"\n",
    "self.groundTruth_imgs_train_dir = f\"{self.original_base_dir}/training/1st_manual/\"\n",
    "self.borderMasks_imgs_train_dir = f\"{self.original_base_dir}/training/mask/\"\n",
    "#test\n",
    "self.original_imgs_test_dir = f\"{self.original_base_dir}/test/images/\"\n",
    "self.groundTruth_imgs_test_dir = f\"{self.original_base_dir}/test/1st_manual/\"\n",
    "self.borderMasks_imgs_test_dir = f\"{self.original_base_dir}/test/mask/\"\n",
    "#------------------------------------------------------------\n",
    "\n",
    "\n",
    "def __get_datasets(\n",
    "    self, \n",
    "    imgs_dir, \n",
    "    groundTruth_dir, \n",
    "    borderMasks_dir, \n",
    "    train_test=\"null\"\n",
    "):\n",
    "    Nimgs = self.Nimgs\n",
    "    channels = self.channels\n",
    "    height = self.height\n",
    "    width = self.width\n",
    "\n",
    "    imgs = np.empty((Nimgs,height,width,channels))\n",
    "    gts = np.empty((Nimgs,height,width))\n",
    "    border_masks = np.empty((Nimgs,height,width))\n",
    "    \n",
    "    for path, subdirs, files in os.walk(imgs_dir): #list all files, directories in the path\n",
    "        for i in range(len(files)):\n",
    "            #original\n",
    "            print (\"original image: \" +files[i])\n",
    "            img = Image.open(imgs_dir+files[i])\n",
    "            imgs[i] = np.asarray(img)\n",
    "            #corresponding ground truth\n",
    "            groundTruth_name = files[i][0:2] + \"_manual1.gif\"\n",
    "            print (\"ground truth name: \" + groundTruth_name)\n",
    "            g_truth = Image.open(groundTruth_dir + groundTruth_name)\n",
    "            gts[i] = np.asarray(g_truth)\n",
    "            #corresponding border masks\n",
    "            border_masks_name = \"\"\n",
    "            if train_test==\"train\":\n",
    "                border_masks_name = files[i][0:2] + \"_training_mask.gif\"\n",
    "            elif train_test==\"test\":\n",
    "                border_masks_name = files[i][0:2] + \"_test_mask.gif\"\n",
    "            else:\n",
    "                print(\"specify if train or test!!\")\n",
    "                exit()\n",
    "            print(\"border masks name: \" + border_masks_name)\n",
    "            b_mask = Image.open(borderMasks_dir + border_masks_name)\n",
    "            border_masks[i] = np.asarray(b_mask)\n",
    "\n",
    "    print(\"imgs max: \"+str(np.max(imgs)))\n",
    "    print(\"imgs min: \"+str(np.min(imgs)))\n",
    "    assert(np.max(gts)==255 and np.max(border_masks)==255)\n",
    "    assert(np.min(gts)==0 and np.min(border_masks)==0)\n",
    "    print(\"ground truth and border masks are correctly withih pixel value range 0-255 (black-white)\")\n",
    "    \n",
    "    #reshaping for my standard tensors\n",
    "    imgs = np.transpose(imgs,(0,3,1,2))\n",
    "    assert(imgs.shape == (Nimgs,channels,height,width))\n",
    "    gts = np.reshape(gts,(Nimgs,1,height,width))\n",
    "    border_masks = np.reshape(border_masks,(Nimgs,1,height,width))\n",
    "    assert(gts.shape == (Nimgs,1,height,width))\n",
    "    assert(border_masks.shape == (Nimgs,1,height,width))\n",
    "    \n",
    "    return imgs, gts, border_masks\n",
    "\n",
    "\n",
    "def prepare_dataset(self):\n",
    "    if not os.path.exists(save_data_dir):\n",
    "        os.makedirs(save_data_dir)\n",
    "\n",
    "    #getting the training datasets\n",
    "    imgs_train, groundTruth_train, border_masks_train = self.__get_datasets(\n",
    "        self.original_imgs_train_dir,\n",
    "        self.groundTruth_imgs_train_dir,\n",
    "        self.borderMasks_imgs_train_dir,\n",
    "        \"train\"\n",
    "    )\n",
    "    print(\"saving train datasets\")\n",
    "    write_hdf5(imgs_train, save_data_dir + \"DRIVE_dataset_imgs_train.hdf5\")\n",
    "    write_hdf5(groundTruth_train, save_data_dir + \"DRIVE_dataset_groundTruth_train.hdf5\")\n",
    "    write_hdf5(border_masks_train,save_data_dir + \"DRIVE_dataset_borderMasks_train.hdf5\")\n",
    "\n",
    "    #getting the testing datasets\n",
    "    imgs_test, groundTruth_test, border_masks_test = self.__get_datasets(\n",
    "        self.original_imgs_test_dir,\n",
    "        self.groundTruth_imgs_test_dir,\n",
    "        self.borderMasks_imgs_test_dir,\n",
    "        \"test\"\n",
    "    )\n",
    "    print(\"saving test datasets\")\n",
    "    write_hdf5(imgs_test,save_data_dir + \"DRIVE_dataset_imgs_test.hdf5\")\n",
    "    write_hdf5(groundTruth_test, save_data_dir + \"DRIVE_dataset_groundTruth_test.hdf5\")\n",
    "    write_hdf5(border_masks_test,save_data_dir + \"DRIVE_dataset_borderMasks_test.hdf5\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-browser",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save patches\n",
    "\n",
    "from help_functions import *\n",
    "from extract_patches import *\n",
    "\n",
    "#function to obtain data for training/testing (validation)\n",
    "from extract_patches import get_data_training\n",
    "\n",
    "#========= Load settings from Config file\n",
    "#patch to the datasets\n",
    "path_data = './DRIVE_datasets_training_testing/'\n",
    "\n",
    "print('extracting patches')\n",
    "patches_imgs_train, patches_masks_train = get_data_training(\n",
    "    DRIVE_train_imgs_original = path_data + 'DRIVE_dataset_imgs_train.hdf5',\n",
    "    DRIVE_train_groudTruth    = path_data + 'DRIVE_dataset_groundTruth_train.hdf5',  #masks\n",
    "    patch_height = 64,\n",
    "    patch_width  = 64,\n",
    "    N_subimgs    = 200000,\n",
    "    inside_FOV = 'True' #select the patches only inside the FOV  (default == True)\n",
    ")\n",
    "\n",
    "np.save('patches_imgs_train',patches_imgs_train)\n",
    "np.save('patches_masks_train',patches_masks_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-genesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing functions\n",
    "\n",
    "from __future__ import division\n",
    "###################################################\n",
    "#\n",
    "#   Script to pre-process the original imgs\n",
    "#\n",
    "##################################################\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "from help_functions import *\n",
    "\n",
    "\n",
    "#My pre processing (use for both training and testing!)\n",
    "def my_PreProc(data):\n",
    "    assert(len(data.shape)==4)\n",
    "    assert (data.shape[1]==3)  #Use the original images\n",
    "    #black-white conversion\n",
    "    train_imgs = rgb2gray(data)\n",
    "    #my preprocessing:\n",
    "    train_imgs = dataset_normalized(train_imgs)\n",
    "    train_imgs = clahe_equalized(train_imgs)\n",
    "    train_imgs = adjust_gamma(train_imgs, 1.2)\n",
    "    train_imgs = train_imgs/255.  #reduce to 0-1 range\n",
    "    return train_imgs\n",
    "\n",
    "\n",
    "#============================================================\n",
    "#========= PRE PROCESSING FUNCTIONS ========================#\n",
    "#============================================================\n",
    "\n",
    "#==== histogram equalization\n",
    "def histo_equalized(imgs):\n",
    "    assert (len(imgs.shape)==4)  #4D arrays\n",
    "    assert (imgs.shape[1]==1)  #check the channel is 1\n",
    "    imgs_equalized = np.empty(imgs.shape)\n",
    "    for i in range(imgs.shape[0]):\n",
    "        imgs_equalized[i,0] = cv2.equalizeHist(np.array(imgs[i,0], dtype = np.uint8))\n",
    "    return imgs_equalized\n",
    "\n",
    "\n",
    "# CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "#adaptive histogram equalization is used. In this, image is divided into small blocks called \"tiles\" (tileSize is 8x8 by default in OpenCV). Then each of these blocks are histogram equalized as usual. So in a small area, histogram would confine to a small region (unless there is noise). If noise is there, it will be amplified. To avoid this, contrast limiting is applied. If any histogram bin is above the specified contrast limit (by default 40 in OpenCV), those pixels are clipped and distributed uniformly to other bins before applying histogram equalization. After equalization, to remove artifacts in tile borders, bilinear interpolation is applied\n",
    "def clahe_equalized(imgs):\n",
    "    assert (len(imgs.shape)==4)  #4D arrays\n",
    "    assert (imgs.shape[1]==1)  #check the channel is 1\n",
    "    #create a CLAHE object (Arguments are optional).\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    imgs_equalized = np.empty(imgs.shape)\n",
    "    for i in range(imgs.shape[0]):\n",
    "        imgs_equalized[i,0] = clahe.apply(np.array(imgs[i,0], dtype = np.uint8))\n",
    "    return imgs_equalized\n",
    "\n",
    "\n",
    "# ===== normalize over the dataset\n",
    "def dataset_normalized(imgs):\n",
    "    assert (len(imgs.shape)==4)  #4D arrays\n",
    "    assert (imgs.shape[1]==1)  #check the channel is 1\n",
    "    imgs_normalized = np.empty(imgs.shape)\n",
    "    imgs_std = np.std(imgs)\n",
    "    imgs_mean = np.mean(imgs)\n",
    "    imgs_normalized = (imgs-imgs_mean)/imgs_std\n",
    "    for i in range(imgs.shape[0]):\n",
    "        imgs_normalized[i] = ((imgs_normalized[i] - np.min(imgs_normalized[i])) / (np.max(imgs_normalized[i])-np.min(imgs_normalized[i])))*255\n",
    "    return imgs_normalized\n",
    "\n",
    "\n",
    "def adjust_gamma(imgs, gamma=1.0):\n",
    "    assert (len(imgs.shape)==4)  #4D arrays\n",
    "    assert (imgs.shape[1]==1)  #check the channel is 1\n",
    "    # build a lookup table mapping the pixel values [0, 255] to\n",
    "    # their adjusted gamma values\n",
    "    invGamma = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "    # apply gamma correction using the lookup table\n",
    "    new_imgs = np.empty(imgs.shape)\n",
    "    for i in range(imgs.shape[0]):\n",
    "        new_imgs[i,0] = cv2.LUT(np.array(imgs[i,0], dtype = np.uint8), table)\n",
    "    return new_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-preference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_dataset():\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-instrumentation",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynapseDataset(Dataset):\n",
    "    def __init__(self, base_dir, list_dir, split, transform=None):\n",
    "        self.transform = transform  # using transform in torch!\n",
    "        self.split = split\n",
    "        self.sample_list = open(os.path.join(list_dir, self.split+'.txt')).readlines()\n",
    "        self.data_dir = base_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.split == \"train\":\n",
    "            slice_name = self.sample_list[idx].strip('\\n')\n",
    "            data_path = os.path.join(self.data_dir, slice_name+'.npz')\n",
    "            data = np.load(data_path)\n",
    "            image, label = data['image'], data['label']\n",
    "        else:\n",
    "            vol_name = self.sample_list[idx].strip('\\n')\n",
    "            filepath = self.data_dir + \"/{}.npy.h5\".format(vol_name)\n",
    "            data = h5py.File(filepath)\n",
    "            image, label = data['image'][:], data['label'][:]\n",
    "\n",
    "        sample = {'image': image, 'label': label}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        sample['case_name'] = self.sample_list[idx].strip('\\n')\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-eating",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-karma",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import show_sbs\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "\n",
    "# # ------------------- params --------------------\n",
    "INPUT_SIZE = 256\n",
    "BASE_DIR = \n",
    "LIST_DIR = \n",
    "\n",
    "# TR_BATCH_SIZE = 8\n",
    "# TR_DL_SHUFFLE = True\n",
    "# TR_DL_WORKER = 1\n",
    "\n",
    "# VL_BATCH_SIZE = 12\n",
    "# VL_DL_SHUFFLE = False\n",
    "# VL_DL_WORKER = 1\n",
    "\n",
    "# TE_BATCH_SIZE = 12\n",
    "# TE_DL_SHUFFLE = False\n",
    "# TE_DL_WORKER = 1\n",
    "# # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "\n",
    "# # ----------------- transform ------------------\n",
    "transform = transforms.Compose([\n",
    "    RandomGenerator(output_size=[INPUT_SIZE, INPUT_SIZE])\n",
    "])\n",
    "# # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "\n",
    "# # ----------------- dataset --------------------\n",
    "# preparing training dataset\n",
    "tr_dataset = SynapseDataset(\n",
    "    base_dir = , \n",
    "    list_dir = , \n",
    "    split=\"train\",\n",
    "    transform=transform\n",
    ")\n",
    "print(\"The length of train set is: {}\".format(len(tr_dataset)))\n",
    "\n",
    "    \n",
    "    \n",
    "# # We consider 1815 samples for training, 259 samples for validation and 520 samples for testing\n",
    "# # !cat ~/deeplearning/skin/Prepare_ISIC2018.py\n",
    "\n",
    "# indices = list(range(len(train_dataset)))\n",
    "\n",
    "# # split indices to: -> train, validation, and test\n",
    "# tr_indices = indices[0:1815]\n",
    "# vl_indices = indices[1815:1815+259]\n",
    "# te_indices = indices[1815+259:2594]\n",
    "\n",
    "# # create new datasets from train dataset as training, validation, and test\n",
    "# tr_dataset = Subset(train_dataset, tr_indices)\n",
    "# vl_dataset = Subset(train_dataset, vl_indices)\n",
    "# te_dataset = Subset(train_dataset, te_indices)\n",
    "\n",
    "import random\n",
    "def worker_init_fn(worker_id):\n",
    "    random.seed(args.seed + worker_id)\n",
    "\n",
    "\n",
    "# # prepare train dataloader\n",
    "trainloader = DataLoader(\n",
    "    tr_dataset, \n",
    "    batch_size=TR_BATCH_SIZE, \n",
    "    shuffle=TR_DL_SHUFFLE, \n",
    "    num_workers=TR_DL_WORKER,\n",
    "    pin_memory=True,\n",
    "    worker_init_fn=worker_init_fn\n",
    ")\n",
    "\n",
    "# # prepare validation dataloader\n",
    "# vl_loader = DataLoader(\n",
    "#     vl_dataset, \n",
    "#     batch_size=VL_BATCH_SIZE, \n",
    "#     shuffle=VL_DL_SHUFFLE, \n",
    "#     num_workers=VL_DL_WORKER,\n",
    "#     pin_memory=True\n",
    "# )\n",
    "\n",
    "# # prepare test dataloader\n",
    "# te_loader = DataLoader(\n",
    "#     te_dataset, \n",
    "#     batch_size=TE_BATCH_SIZE, \n",
    "#     shuffle=TE_DL_SHUFFLE, \n",
    "#     num_workers=TE_DL_WORKER,\n",
    "#     pin_memory=True\n",
    "# )\n",
    "\n",
    "# -------------- test -----------------\n",
    "# test and visualize the input data\n",
    "for batch in tr_loader:\n",
    "    print(\"Training\")\n",
    "    img = batch['image']\n",
    "    msk = batch['label']\n",
    "    show_sbs(img[0], msk[0])\n",
    "    break\n",
    "    \n",
    "# for img, msk in vl_loader:\n",
    "#     print(\"Validation\")\n",
    "#     show_sbs(img[0], msk[0])\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cuda11",
   "language": "python",
   "name": "pytorch_cuda11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
